{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEEe_S5Jt1eK"
      },
      "source": [
        "# SFT Training\n",
        "\n",
        "This notebook trains a supervised fine-tuning (SFT) baseline to compare against GRPO results.\n",
        "Uses the same data, hyperparameters, and system prompt as the GRPO experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Unsloth"
      ],
      "metadata": {
        "id": "7-MOOi_XGOUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LhzQmQX0KRT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab!\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFBsAUXNA9rJ"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import wandb"
      ],
      "metadata": {
        "id": "bmJLjMeqGRdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScgNw4tpDfby",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "23d81853-77b2-4236-e95a-eff357177d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msigo444\u001b[0m (\u001b[33msigo444-university-of-southern-denmark\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251006_215032-d3banp28</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sigo444-university-of-southern-denmark/gsm8k-prolog-prover/runs/d3banp28' target=\"_blank\">sft-sp-reflect-baseline</a></strong> to <a href='https://wandb.ai/sigo444-university-of-southern-denmark/gsm8k-prolog-prover' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sigo444-university-of-southern-denmark/gsm8k-prolog-prover' target=\"_blank\">https://wandb.ai/sigo444-university-of-southern-denmark/gsm8k-prolog-prover</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sigo444-university-of-southern-denmark/gsm8k-prolog-prover/runs/d3banp28' target=\"_blank\">https://wandb.ai/sigo444-university-of-southern-denmark/gsm8k-prolog-prover/runs/d3banp28</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sigo444-university-of-southern-denmark/gsm8k-prolog-prover/runs/d3banp28?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c8b3c66b140>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "wandb.init(\n",
        "    project=\"gsm8k-prolog-prover\",\n",
        "    name=\"sft-sp-reflect\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "Hqw9DDya8HO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYvRXkiG0YXc",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4027dd77-380b-481d-df14-9467367ce6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.8.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Unsloth: Xformers was not installed correctly.\n",
            "Please install xformers separately first.\n",
            "Then confirm if it's correctly installed by running:\n",
            "python -m xformers.info\n",
            "\n",
            "Longer error message:\n",
            "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.8.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.10.1: Fast Qwen2 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.10.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import is_bfloat16_supported, FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    fast_inference = False,  # SFT doesn't need vLLM\n",
        "    max_lora_rank = 64,\n",
        "    gpu_memory_utilization = 0.7,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 64,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_EZ49Agx1M"
      },
      "source": [
        "### System prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec9SEnHyVb4G"
      },
      "outputs": [],
      "source": [
        "# sp-reflect\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a specialized Prolog code-generating assistant.\n",
        "\n",
        "Your task is to solve math problems by providing a structured answer in two clearly defined sections:\n",
        "\n",
        "1. <reasoning>\n",
        "   - Provide a clear, concise step-by-step explanation of how you arrive at the solution.\n",
        "   - Review the reasoning at the end of the <reasoning> section to ensure that all computations and logical deductions are correct.\n",
        "   - If something is not correct, then try again: Provide a clear, concise step-by-step explanation of how you arrive at the solution.\n",
        "\n",
        "2. <answer>\n",
        "   - Provide executable Prolog code using constraint logic programming to compute the numeric answer.\n",
        "   - Always start with: ':- use_module(library(clpq)).'\n",
        "   - Define any necessary numeric constants or intermediate values using predicates.\n",
        "   - Final answer should be unified explicitly in solve(X) using curly-brace constraints, without printing commands.\n",
        "\n",
        "Use this XML format strictly:\n",
        "<reasoning>\n",
        "- Your step-by-step reasoning here\n",
        "- Your review of the reasoning here\n",
        "- Your potential further step-by-step reasoning here\n",
        "</reasoning>\n",
        "<answer>\n",
        ":- use_module(library(clpq)).\n",
        "\n",
        "(Any predicates/constants defined here)\n",
        "\n",
        "solve(X) :-\n",
        "    (Intermediate computations using curly braces)\n",
        "    {X = final constraint logic}.\n",
        "</answer>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZop2dGohEsy"
      },
      "source": [
        "### Load and format dataset for SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIW598Nh0f57"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "def get_gsm8k_split(subset_size=2500, seed=42):\n",
        "    \"\"\"\n",
        "    Load dataset and split into 70% train, 15% validation, 15% test.\n",
        "    Same split as GRPO experiments.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"niklasm222/gsm8k-prolog-prover-sp_reflect-v8.2\", split=\"train\")\n",
        "    subset = dataset.shuffle(seed=seed).select(range(subset_size))\n",
        "\n",
        "    # Split off 15% for test\n",
        "    split_1 = subset.train_test_split(test_size=0.15, seed=seed)\n",
        "    train_val = split_1[\"train\"]\n",
        "    test = split_1[\"test\"]\n",
        "\n",
        "    # From remaining 85%, split off 15% for validation\n",
        "    val_ratio = 0.15 / 0.85\n",
        "    split_2 = train_val.train_test_split(test_size=val_ratio, seed=seed)\n",
        "    train = split_2[\"train\"]\n",
        "    val = split_2[\"test\"]\n",
        "\n",
        "    return DatasetDict({\"train\": train, \"validation\": val, \"test\": test})\n",
        "\n",
        "# Load Data\n",
        "splits = get_gsm8k_split()\n",
        "train_dataset = splits[\"train\"]\n",
        "val_dataset = splits[\"validation\"]\n",
        "test_dataset = splits[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format dataset for SFT"
      ],
      "metadata": {
        "id": "AilVh2fl5pL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "    \"\"\"\n",
        "    Format examples for SFT training.\n",
        "    Wraps the reference Prolog code in <answer> tags.\n",
        "    Note: We only supervise the <answer> section since we don't have\n",
        "    ground-truth reasoning steps in the dataset.\n",
        "    \"\"\"\n",
        "    # Create the complete conversation with assistant response\n",
        "    messages = example[\"prompt\"] + [\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": f\"<answer>\\n{example['output']}\\n</answer>\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Format all splits\n",
        "train_dataset_formatted = train_dataset.map(\n",
        "    formatting_func,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"\\nExample formatted training sample:\")\n",
        "print(train_dataset_formatted[0][\"text\"])"
      ],
      "metadata": {
        "id": "8TouRF52vilH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SFTConfig and SFTTrainer"
      ],
      "metadata": {
        "id": "uF4n7xGh5INA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CBhiRud03uD"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    seed=42,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=250,\n",
        "    max_grad_norm=0.1,\n",
        "    max_seq_length=2048,\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=\"outputs_sft\",\n",
        "    dataset_text_field=\"text\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqTcnqzSnWz_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_formatted,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA adapter properly\n",
        "model.save_pretrained(\"sft_saved_lora\")\n",
        "tokenizer.save_pretrained(\"sft_saved_lora\")\n",
        "\n",
        "# Merge to 16bit\n",
        "if True:\n",
        "    model.save_pretrained_merged(\n",
        "        \"qwen2.5-3b-sft-1.75k-gsm8k-sp-reflect\",\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\"\n",
        "    )\n",
        "\n",
        "if True:\n",
        "    model.push_to_hub_merged(\n",
        "        \"niklasm222/qwen2.5-3b-sft-1.75k-gsm8k-sp-reflect\",\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\",\n",
        "        token=\"\"\n",
        "    )"
      ]
    }
  ]
}
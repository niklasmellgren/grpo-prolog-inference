{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEEe_S5Jt1eK"
      },
      "source": [
        "This is a modular notebook that trains a supervised fine-tuning (SFT) baseline to compare against GRPO results.\n",
        "We use the same data, hyperparameters, and system prompt as the GRPO experiments.\n",
        "\n",
        "The system prompt (currently sp-reflect) can be replaced by sp-base, sp-struct, and sp-declare, and the dataset can be replaced to pair with the correct system prompt." 
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Unsloth"
      ],
      "metadata": {
        "id": "7-MOOi_XGOUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LhzQmQX0KRT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab!\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFBsAUXNA9rJ"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import wandb"
      ],
      "metadata": {
        "id": "bmJLjMeqGRdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScgNw4tpDfby",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "wandb.init(\n",
        "    project=\"gsm8k-prolog-prover\",\n",
        "    name=\"sft-sp-reflect\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "Hqw9DDya8HO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYvRXkiG0YXc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from unsloth import is_bfloat16_supported, FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    fast_inference = False,  # SFT doesn't need vLLM\n",
        "    max_lora_rank = 64,\n",
        "    gpu_memory_utilization = 0.7,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 64,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_EZ49Agx1M"
      },
      "source": [
        "### System prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec9SEnHyVb4G"
      },
      "outputs": [],
      "source": [
        "# sp-reflect\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a specialized Prolog code-generating assistant.\n",
        "\n",
        "Your task is to solve math problems by providing a structured answer in two clearly defined sections:\n",
        "\n",
        "1. <reasoning>\n",
        "   - Provide a clear, concise step-by-step explanation of how you arrive at the solution.\n",
        "   - Review the reasoning at the end of the <reasoning> section to ensure that all computations and logical deductions are correct.\n",
        "   - If something is not correct, then try again: Provide a clear, concise step-by-step explanation of how you arrive at the solution.\n",
        "\n",
        "2. <answer>\n",
        "   - Provide executable Prolog code using constraint logic programming to compute the numeric answer.\n",
        "   - Always start with: ':- use_module(library(clpq)).'\n",
        "   - Define any necessary numeric constants or intermediate values using predicates.\n",
        "   - Final answer should be unified explicitly in solve(X) using curly-brace constraints, without printing commands.\n",
        "\n",
        "Use this XML format strictly:\n",
        "<reasoning>\n",
        "- Your step-by-step reasoning here\n",
        "- Your review of the reasoning here\n",
        "- Your potential further step-by-step reasoning here\n",
        "</reasoning>\n",
        "<answer>\n",
        ":- use_module(library(clpq)).\n",
        "\n",
        "(Any predicates/constants defined here)\n",
        "\n",
        "solve(X) :-\n",
        "    (Intermediate computations using curly braces)\n",
        "    {X = final constraint logic}.\n",
        "</answer>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZop2dGohEsy"
      },
      "source": [
        "### Load and format dataset for SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIW598Nh0f57"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "def get_gsm8k_split(subset_size=2500, seed=42):\n",
        "    \"\"\"\n",
        "    Load dataset and split into 70% train, 15% validation, 15% test.\n",
        "    Same split as GRPO experiments.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"niklasm222/gsm8k-prolog-prover-sp_reflect-v8.2\", split=\"train\")\n",
        "    subset = dataset.shuffle(seed=seed).select(range(subset_size))\n",
        "\n",
        "    # Split off 15% for test\n",
        "    split_1 = subset.train_test_split(test_size=0.15, seed=seed)\n",
        "    train_val = split_1[\"train\"]\n",
        "    test = split_1[\"test\"]\n",
        "\n",
        "    # From remaining 85%, split off 15% for validation\n",
        "    val_ratio = 0.15 / 0.85\n",
        "    split_2 = train_val.train_test_split(test_size=val_ratio, seed=seed)\n",
        "    train = split_2[\"train\"]\n",
        "    val = split_2[\"test\"]\n",
        "\n",
        "    return DatasetDict({\"train\": train, \"validation\": val, \"test\": test})\n",
        "\n",
        "# Load Data\n",
        "splits = get_gsm8k_split()\n",
        "train_dataset = splits[\"train\"]\n",
        "val_dataset = splits[\"validation\"]\n",
        "test_dataset = splits[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format dataset for SFT"
      ],
      "metadata": {
        "id": "AilVh2fl5pL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "    \"\"\"\n",
        "    Format examples for SFT training.\n",
        "    Wraps the reference Prolog code in <answer> tags.\n",
        "    Note: We only supervise the <answer> section since we don't have\n",
        "    ground-truth reasoning steps in the dataset.\n",
        "    \"\"\"\n",
        "    # Create the complete conversation with assistant response\n",
        "    messages = example[\"prompt\"] + [\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": f\"<answer>\\n{example['output']}\\n</answer>\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Format all splits\n",
        "train_dataset_formatted = train_dataset.map(\n",
        "    formatting_func,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"\\nExample formatted training sample:\")\n",
        "print(train_dataset_formatted[0][\"text\"])"
      ],
      "metadata": {
        "id": "8TouRF52vilH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SFTConfig and SFTTrainer"
      ],
      "metadata": {
        "id": "uF4n7xGh5INA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CBhiRud03uD"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    seed=42,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=250,\n",
        "    max_grad_norm=0.1,\n",
        "    max_seq_length=2048,\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=\"outputs_sft\",\n",
        "    dataset_text_field=\"text\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqTcnqzSnWz_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_formatted,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA adapter properly\n",
        "model.save_pretrained(\"sft_saved_lora\")\n",
        "tokenizer.save_pretrained(\"sft_saved_lora\")\n",
        "\n",
        "# Merge to 16bit\n",
        "if True:\n",
        "    model.save_pretrained_merged(\n",
        "        \"qwen2.5-3b-sft-1.75k-gsm8k-sp-reflect\",\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\"\n",
        "    )\n",
        "\n",
        "if True:\n",
        "    model.push_to_hub_merged(\n",
        "        \"niklasm222/qwen2.5-3b-sft-1.75k-gsm8k-sp-reflect\",\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\",\n",
        "        token=\"\"\n",
        "    )"
      ]
    }
  ]
}
